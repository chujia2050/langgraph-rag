{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e44672b-dafd-4fec-a97e-c3b96bd9dae3",
   "metadata": {},
   "source": [
    "<h1>Self-RAG</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb00dfa8-f9ab-42da-8215-85a282b563f5",
   "metadata": {},
   "source": [
    "<h2>Setup</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "b341df0d-4a84-445f-b70b-4f8ae4fbf5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7e86794d-c6da-47b2-b6de-aa1cb5b9e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d6d2c6f0-f31e-4d10-a36a-53cbd29481ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 8, 'total_tokens': 18, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb9dce56a8', 'finish_reason': 'stop', 'logprobs': None}, id='run-e9e64828-6cc5-47e4-b82a-989a5eef2a9c-0', usage_metadata={'input_tokens': 8, 'output_tokens': 10, 'total_tokens': 18, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "response = llm.invoke(\"Hi\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62b056e-1601-4f38-80ed-ebf6ec881e9b",
   "metadata": {},
   "source": [
    "<h2>Retriever</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7989b9e6-d012-43b4-af9e-7d156a7a415c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "urls = [\n",
    "    \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
    "    \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
    "]\n",
    "\n",
    "docs = [WebBaseLoader(url).load() for url in urls] #Load documents\n",
    "docs_list = [item for sublist in docs for item in sublist] #Flatten the list\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=250, chunk_overlap=0\n",
    ")\n",
    "doc_splits = text_splitter.split_documents(docs_list)\n",
    "\n",
    "# Add to vectorDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chroma\",\n",
    "    embedding=embeddings,\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "80ff0a20-f728-4c33-a99c-ac13549af4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "question = \"agent memory\"\n",
    "documents = retriever.invoke(question) #the default number of retrieved documents (k) is 4, return a list of documents\n",
    "type(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "803e92ca-f5b1-45d8-a460-9c40349a928c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a6a198a3-12ee-44e2-b7f2-b2c269cead58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ad9402-5c40-46c2-84aa-f38d468caa02",
   "metadata": {},
   "source": [
    "<h2>Documents Grader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8a67f56b-25d8-4746-a4bc-c3f10fca695e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: int = Field(\n",
    "        description=\"Documents are relevant to the question, 0 or 1\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are a grader assessing relevance of a retrieved document to a user question. \n",
    "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 0 or 1 score to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "human_msg = \"\"\"\n",
    "    Retrieved documents: \n",
    "    {documents} \n",
    "    \n",
    "    User question: \n",
    "    {question}\"\"\" \n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human_msg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c711fd29-a96d-4edf-9af9-9752da7f3910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "You are a grader assessing relevance of a retrieved document to a user question. \n",
      "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
      "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
      "Give a binary score 0 or 1 score to indicate whether the document is relevant to the question.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "    Retrieved documents: \n",
      "    \u001b[33;1m\u001b[1;3m{documents}\u001b[0m \n",
      "    \n",
      "    User question: \n",
      "    \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "grade_prompt.pretty_print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "426aa54e-054c-4b66-95bd-da1efa7e9230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.GradeDocuments"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "score = retrieval_grader.invoke({\"question\": question, \"documents\": documents}) #We can pass a list of documents, a single document or a string\n",
    "type(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9fdbb4be-74b6-4f6f-a325-07663bcacb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.binary_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703f2d1-b2e3-473e-9d43-6425b9eb69bb",
   "metadata": {},
   "source": [
    "<h2>Generate Answer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "ee1c7443-19f4-4c68-bcef-a8c25c55d7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\") #prompt has context and question parameter\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "29e35bfc-56b2-4a9d-a4bd-84f0b65a280c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: \u001b[33;1m\u001b[1;3m{question}\u001b[0m \n",
      "Context: \u001b[33;1m\u001b[1;3m{context}\u001b[0m \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt.pretty_print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f327222c-851e-4e07-b8dd-f57711b9ada8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In an LLM-powered autonomous agent system, memory is divided into short-term and long-term components. Short-term memory involves in-context learning, while long-term memory allows the agent to retain and recall information over extended periods, often using an external vector store for fast retrieval. This memory system enables the agent to learn from past actions and improve future performance.\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b58281b-9bc6-4829-8d8b-ac2f14ed6860",
   "metadata": {},
   "source": [
    "<h2>Hallucination Grader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "49736670-d757-4d92-b39e-e7ad6ad35ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "\n",
    "    binary_score: int = Field(\n",
    "        description=\"Answer is grounded in the facts, 0 or 1\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\n",
    "Give a binary score 1 or 0. 1 means that the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "human_msg = \"\"\"\n",
    "    Set of facts: \n",
    "    {documents} \n",
    "    \n",
    "    LLM generation: \n",
    "    {generation}\"\"\"\n",
    "    \n",
    "hallucination_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human_msg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader = hallucination_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "766d11c7-4dd5-4f90-a178-668dc33a06e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "You are a grader assessing relevance of a retrieved document to a user question. \n",
      "It does not need to be a stringent test. The goal is to filter out erroneous retrievals. \n",
      "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
      "Give a binary score 0 or 1 score to indicate whether the document is relevant to the question.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "    Retrieved documents: \n",
      "    \u001b[33;1m\u001b[1;3m{documents}\u001b[0m \n",
      "    \n",
      "    User question: \n",
      "    \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "grade_prompt.pretty_print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "6ee46899-7ed6-4cfb-a511-667d1d1c4f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.GradeHallucinations"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "score = hallucination_grader.invoke({\"documents\": documents, \"generation\": generation})\n",
    "type(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "be4bbe1d-df09-4dde-8452-62482f310fc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.binary_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a99354-9a6c-4938-8843-d497c33fee90",
   "metadata": {},
   "source": [
    "<h2>Answer Grader</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "4aa64c5c-3be7-4d6d-b8c7-9bf836f04215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data model\n",
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score: int = Field(\n",
    "        description=\"Answer addresses the question, 1 or 0\"\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM with function call\n",
    "structured_llm_grader = llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You are a grader assessing whether an answer addresses / resolves a question.\n",
    "Give a binary score 1 or 0. 1 means that the answer resolves the question.\"\"\"\n",
    "\n",
    "human_msg = \"\"\"\n",
    "    User question:\n",
    "    {question} \n",
    "    \n",
    "    LLM generation:\n",
    "    {generation}\"\"\" \n",
    "\n",
    "answer_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human_msg),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader = answer_prompt | structured_llm_grader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e73670e5-bb53-446b-898f-4e72e2369d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.GradeAnswer"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test \n",
    "score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "type(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "a480a813-707b-4845-b540-b42882330ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score.binary_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d144ad00-db3f-489b-b102-2a987eb1ae2c",
   "metadata": {},
   "source": [
    "<h2>Question Re-writer</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "86690705-5bb1-4299-805d-8e85661c6d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prompt\n",
    "system = \"\"\"\n",
    "You a question re-writer that converts an input question to a better version that is optimized for web search. \n",
    "Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "\n",
    "human_msg = \"\"\"\n",
    "    Look at the input and try to reason about the underlying semantic intent / meaning.\n",
    "    \n",
    "    Here is the initial question: \n",
    "    {question}\n",
    "    \n",
    "    Formulate an improved question.\"\"\" \n",
    "\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", human_msg)\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a7c9b477-b574-429a-94fc-f17920dbd959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "\n",
      "You a question re-writer that converts an input question to a better version that is optimized for web search. \n",
      "Look at the input and try to reason about the underlying semantic intent / meaning.\n",
      "\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "\n",
      "    Look at the input and try to reason about the underlying semantic intent / meaning.\n",
      "    \n",
      "    Here is the initial question: \n",
      "    \u001b[33;1m\u001b[1;3m{question}\u001b[0m\n",
      "    \n",
      "    Formulate an improved question.\n"
     ]
    }
   ],
   "source": [
    "re_write_prompt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "be3f58a1-3863-4cfc-9f22-4d434763dfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is agent memory and how does it function in artificial intelligence systems?\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "better_question = question_rewriter.invoke({\"question\": question})\n",
    "print(better_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7330010b-8481-4d5b-aec2-48d1947dd663",
   "metadata": {},
   "source": [
    "<h2>Graph State</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a3dcb7de-6dfa-4474-888f-2cf5122a0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain.schema import Document\n",
    "\n",
    "class State(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "    question: str\n",
    "    generation: str\n",
    "    documents: List[Document]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d4c7fa-4daa-4244-b02d-7673cc205241",
   "metadata": {},
   "source": [
    "<h2>Retriever Node</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e8dac85b-7b60-4a90-badb-f7127d3f6143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    \n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f84536-71e9-4a54-bd4d-373a9f089213",
   "metadata": {},
   "source": [
    "<h2>Documents Grader Node</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7926a37-9ae1-4b33-bb8c-ac4696a51bbf",
   "metadata": {},
   "source": [
    "If any docs are relevant, we can proceed with generating answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "bfd48dae-56e1-4d64-ab49-e35683c0a206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    for doc in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"documents\": doc}\n",
    "        )\n",
    "        grade = score.binary_score \n",
    "        if grade == 1:\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            \n",
    "    return {\"documents\": filtered_docs}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9bd87b-0d08-4e68-bb9e-12c1edc1ebd5",
   "metadata": {},
   "source": [
    "<h2>Generate Node</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8686c16e-b6dc-446b-b3f5-088ccf6b8a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, generation, that contains LLM generation\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # RAG generation\n",
    "    generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    \n",
    "    return {\"generation\": generation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccce48a6-674c-4e16-aebd-ed2e6740c098",
   "metadata": {},
   "source": [
    "<h2>Question Re-writer Node</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "91bd7abb-593c-409d-a188-ed51445c3090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---Rewrite---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    \n",
    "    return {\"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf3a0d8-d533-4f0a-be6b-588a3cf2612e",
   "metadata": {},
   "source": [
    "<h2>Conditional Edge</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "faf62f51-bc85-4b73-b8c5-9f134c8f3dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def decide_to_generate(state) -> Literal[\"generate\", \"rewrite\"]:\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or rewrite a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    filtered_documents = state[\"documents\"]\n",
    "\n",
    "    if not filtered_documents:\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, REWRITE QUESTION---\"\n",
    "        )\n",
    "        return \"rewrite\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "e954532a-b00a-4800-9563-b317051402ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_answer(state) -> Literal[\"useful\", \"not useful\", \"not supported\"]:\n",
    "    \"\"\"\n",
    "    Determines whether the generation is grounded in the document and answers question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK HALLUCINATIONS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    generation = state[\"generation\"]\n",
    "\n",
    "    hallucination_score = hallucination_grader.invoke(\n",
    "        {\"documents\": documents, \"generation\": generation}\n",
    "    )\n",
    "    hallucination_grade = hallucination_score.binary_score\n",
    "\n",
    "    # Check hallucination\n",
    "    if hallucination_grade == 1:\n",
    "        print(\"---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\")\n",
    "        # Check question-answering\n",
    "        print(\"---GRADE GENERATION vs QUESTION---\")\n",
    "        answer_score = answer_grader.invoke({\"question\": question, \"generation\": generation})\n",
    "        answer_grade = answer_score.binary_score\n",
    "        if answer_grade == 1:\n",
    "            print(\"---DECISION: GENERATION ADDRESSES QUESTION---\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"---DECISION: GENERATION DOES NOT ADDRESS QUESTION---\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"---DECISION: GENERATION IS NOT GROUNDED IN DOCUMENTS, RE-TRY---\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b54cfc-c1af-4f27-adb8-f155109ee137",
   "metadata": {},
   "source": [
    "<h2>Build Graph</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "80019ec1-f6eb-47ed-ac95-6e803cff8307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "workflow = StateGraph(State)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade\", grade)  # grade\n",
    "workflow.add_node(\"generate\", generate)  # generatae\n",
    "workflow.add_node(\"rewrite\", rewrite)  # rewrite\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"rewrite\": \"rewrite\",\n",
    "        \"generate\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"rewrite\", \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    decide_to_answer,\n",
    "    {\n",
    "        \"useful\": END,\n",
    "        \"not useful\": \"rewrite\",\n",
    "        \"not supported\": \"generate\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compile\n",
    "graph = workflow.compile()\n",
    "\n",
    "# View\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e04416b1-173d-4aa2-8a7f-49a77edd2232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Output from node 'retrieve':\"\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).')]}\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Output from node 'grade':\"\n",
      "{ 'documents': [ Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Short-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.'),\n",
      "                 Document(metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory\\n\\nShort-term memory: I would consider all the in-context learning (See Prompt Engineering) as utilizing short-term memory of the model to learn.\\nLong-term memory: This provides the agent with the capability to retain and recall (infinite) information over extended periods, often by leveraging an external vector store and fast retrieval.\\n\\n\\nTool use\\n\\nThe agent learns to call external APIs for extra information that is missing from the model weights (often hard to change after pre-training), including current information, code execution capability, access to proprietary information sources and more.\\n\\n\\n\\n\\nFig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"}, page_content='Fig. 7. Comparison of AD, ED, source policy and RL^2 on environments that require memory and exploration. Only binary reward is assigned. The source policies are trained with A3C for \"dark\" environments and DQN for watermaze.(Image source: Laskin et al. 2023)\\nComponent Two: Memory#\\n(Big thank you to ChatGPT for helping me draft this section. I’ve learned a lot about the human brain and data structure for fast MIPS in my conversations with ChatGPT.)\\nTypes of Memory#\\nMemory can be defined as the processes used to acquire, store, retain, and later retrieve information. There are several types of memory in human brains.\\n\\n\\nSensory Memory: This is the earliest stage of memory, providing the ability to retain impressions of sensory information (visual, auditory, etc) after the original stimuli have ended. Sensory memory typically only lasts for up to a few seconds. Subcategories include iconic memory (visual), echoic memory (auditory), and haptic memory (touch).')]}\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Output from node 'generate':\"\n",
      "{ 'generation': 'In an LLM-powered autonomous agent system, short-term memory '\n",
      "                'involves in-context learning, allowing the model to learn and '\n",
      "                'adapt within the context of a given task. Long-term memory '\n",
      "                'enables the agent to retain and recall information over '\n",
      "                'extended periods, often using an external vector store for '\n",
      "                'efficient retrieval. These memory types help the agent handle '\n",
      "                'complex tasks by learning from past actions and retaining '\n",
      "                'necessary information.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\"question\": \"Explain how the different types of agent memory work?\"}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3d70eee9-f886-4b11-8439-255e4a0a60e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---RETRIEVE---\n",
      "\"Output from node 'retrieve':\"\n",
      "{ 'documents': [ Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).')]}\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Output from node 'grade':\"\n",
      "{ 'documents': [ Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='Use an iterative Monte Carlo search method to improve the best candidates by proposing semantically similar variants via prompts like Generate a variation of the following instruction while keeping the semantic meaning.\\\\n\\\\nInput: ...\\\\n\\\\nOutput:...\\n\\n\\nTo construct chain-of-thought prompts automatically, Shum et al. (2023) suggested augment-prune-select, a three-step process:\\n\\nAugment: Generate multiple pseudo-chains of thought given question using few-shot or zero-shot CoT prompts;\\nPrune: Prune pseudo chains based on whether generated answers match ground truths.\\nSelect: Apply a variance-reduced policy gradient strategy to learn the probability distribution over selected examples, while considering the probability distribution over examples as policy and the validation set accuracy as reward.'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).'),\n",
      "                 Document(metadata={'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\"}, page_content='[8] Wang et al. “Self-Consistency Improves Chain of Thought Reasoning in Language Models.” ICLR 2023.\\n[9] Diao et al. “Active Prompting with Chain-of-Thought for Large Language Models.” arXiv preprint arXiv:2302.12246 (2023).\\n[10] Zelikman et al. “STaR: Bootstrapping Reasoning With Reasoning.” arXiv preprint arXiv:2203.14465 (2022).\\n[11] Ye & Durrett. “The unreliability of explanations in few-shot in-context learning.” arXiv preprint arXiv:2205.03401 (2022).\\n[12] Trivedi et al. “Interleaving retrieval with chain-of-thought reasoning for knowledge-intensive multi-step questions.” arXiv preprint arXiv:2212.10509 (2022).')]}\n",
      "---GENERATE---\n",
      "---CHECK HALLUCINATIONS---\n",
      "---DECISION: GENERATION IS GROUNDED IN DOCUMENTS---\n",
      "---GRADE GENERATION vs QUESTION---\n",
      "---DECISION: GENERATION ADDRESSES QUESTION---\n",
      "\"Output from node 'generate':\"\n",
      "{ 'generation': 'Chain of thought prompting involves generating multiple '\n",
      "                'reasoning paths (pseudo-chains of thought) for a given '\n",
      "                'question using few-shot or zero-shot prompts. These paths are '\n",
      "                'then pruned based on their accuracy in matching ground truth '\n",
      "                'answers. Finally, a policy gradient strategy is used to '\n",
      "                'select the most effective examples, optimizing for validation '\n",
      "                'set accuracy.'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "inputs = {\"question\": \"Explain how chain of thought prompting works?\"}\n",
    "for output in graph.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        pprint.pprint(f\"Output from node '{key}':\")\n",
    "        pprint.pprint(value, indent=2, width=80, depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a45223-80f5-4edc-9ac6-52c4d81cdc86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
